{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Implementation of a Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coding a neural network from scratch is a great way to understand how they work. In this notebook, I will implement a neural network from scratch with basic python libraries such as **numpy** and **matplotlib**. There will also be implementations of **forward pass**, **backward pass**, and **gradient descent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
      "Numpy version: 1.21.5\n",
      "Matplotlib version: 3.5.2\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib as plt\n",
    "\n",
    "def versions():\n",
    "    print(\"Python version: {}\".format(sys.version))\n",
    "    print(\"Numpy version: {}\".format(np.__version__))\n",
    "    print(\"Matplotlib version: {}\".format(plt._version.version)) # type: ignore\n",
    "    \n",
    "versions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuron which is a part of the **hidden layer** of a neural network is a mathematical function that takes in some inputs, performs some calculations on them, and produces an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 44.64\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.5, 3.7]\n",
    "weights = [4.5 , 2.1, 8.7]\n",
    "\n",
    "bias = 2.7      # There is only one bias for a neuron.\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "\n",
    "output = round(output, 3)       # Reducing the size of the output\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We could define a function for the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function of a single neuron with any input size\n",
    "\n",
    "def neuron(inputs, weights, bias):\n",
    "    output = 0\n",
    "    for i in range(len(inputs)):\n",
    "        output += inputs[i]*weights[i]\n",
    "    output += bias\n",
    "    \n",
    "    return round(output, 3)\n",
    "\n",
    "neuron(\n",
    "    [1.0, 2.5, 3.7],\n",
    "    [4.5 , 2.1, 8.7],\n",
    "    4.7\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A layer is a **collection of neurons**. We can define a layer with specific amount of neurons with specific amount of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.64, 32.52, 47.96]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer(inputs, weights, biases):     # A layer is a collection of neurons\n",
    "    output = []\n",
    "    for i in range(len(biases)):\n",
    "        output.append(neuron(inputs, weights[i], biases[i]))\n",
    "        \n",
    "    return output  # Returns a list of outputs from each neuron in the layer\n",
    "\n",
    "layer(\n",
    "    [1.0, 2.5, 3.7],\n",
    "    [[4.5 , 2.1, 8.7], [1.2, 3.4, 5.6], [7.8, 9.1, 2.3]],   # 3 neurons with 3 weights each\n",
    "    [4.7, 2.1, 8.9]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This layer function can be improved with extra parameters that describes the layer in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.14, 39.02, 72.96, 53.82]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer(neuron_count, inputs, weights, biases):\n",
    "    output = []\n",
    "    for i in range(neuron_count):\n",
    "        try:\n",
    "            output.append(neuron(inputs, weights[i], biases[i]))\n",
    "        except IndexError:\n",
    "            print(\"Size of weights, inputs and biases must be equal to the number of neurons!\")\n",
    "            break\n",
    "    return output  # Returns a list of outputs\n",
    "\n",
    "layer(\n",
    "    4,  # Number of neurons\n",
    "    [1.0, 2.5, 3.7, 5.0],   # Number of inputs\n",
    "    [[4.5, 2.1, 8.7, 0.5], [1.2, 3.4, 5.6, 1.3], [7.8, 9.1, 2.3, 5,3], [8.8, 9.9, 1.1, 3,3]],   # 4 neurons with 4 weights each\n",
    "    [4.7, 2.1, 8.9, 1.2]   # 4 neurons with 1 bias each\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's possible to define a layer using numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36.14, 30.12, 30.94])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_numpy(inputs, weights, biases):\n",
    "    return np.dot(weights, inputs) + biases\n",
    "\n",
    "layer_numpy(\n",
    "    [1.0, 2.5, 3.7],\n",
    "    [[4.5 , -2.1, 8.7], [-1.2, 3.4, 5.6], [7.8, 9.1, -2.3],],   # 3 neurons with 3 weights each\n",
    "    [4.7, 2.1, 8.9]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch of Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple inputs can be passed to the layer at once. This is called a **batch**. A batch is a collection of inputs. **Batch size** effects the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of samples: (3, 4)\n",
      "Shape of weights: (3, 4)\n",
      "Shape of biases: (3,)\n"
     ]
    }
   ],
   "source": [
    "sample = [1.0, 2.0, 3.0, 2.5]   # Single sample with 4 features.\n",
    "\n",
    "samples = [\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],  # 3 samples with 4 features each.\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],   # 4 neurons with 4 weights each.\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]    # 3 neurons with 1 bias each.\n",
    "\n",
    "print(\"Shape of samples: {}\\nShape of weights: {}\\nShape of biases: {}\"\n",
    "      .format(np.shape(samples), np.shape(weights), np.shape(biases)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To be able to get the dot product of the batch and the weights, we need to transpose the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of samples: (3, 4)\n",
      "Shape of weights: (4, 3)\n",
      "Shape of biases: (3,)\n"
     ]
    }
   ],
   "source": [
    "weights = np.array(weights).T   # Transposing the weights matrix.\n",
    "print(\"Shape of samples: {}\\nShape of weights: {}\\nShape of biases: {}\"\n",
    "      .format(np.shape(samples), np.shape(weights), np.shape(biases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.64\n",
      "Shape of the first layer's output: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "output_l1 = np.dot(samples, weights) + biases   # Output of the first layer.\n",
    "print(f\"{output}\\nShape of the first layer's output: {np.shape(output_l1)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Another Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Layer\n",
    "weights_2 = [\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],   # 3 neurons with 3 weights each.\n",
    "    [-0.44, 0.73, -0.13]\n",
    "]\n",
    "\n",
    "biases_2 = [-1.0, 2.0, -0.5]    # 3 neurons with 1 bias each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n",
      "Shape of the second layer's output: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "weights_2 = np.array(weights_2).T   # Transposing the weights matrix.\n",
    "\n",
    "output_l2 = np.dot(output_l1, weights_2) + biases_2 # Output of the second layer.\n",
    "print(f\"{output_l2}\\nShape of the second layer's output: {np.shape(output_l2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layers can be defined as an object with a **forward** method. This method takes in a batch of inputs and returns a batch of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = [\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)  # Random weights with the same shape as the input and the number of neurons.\n",
    "        self.biases = np.zeros((1, n_neurons))  # Biases with the shape of (1, n_neurons)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer_Dense(4, 5)\n",
    "l2 = Layer_Dense(5, 2)\n",
    "\n",
    "l1.forward(X)\n",
    "\n",
    "l2.forward(l1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the first layer: \n",
      "[[-0.27675317 -0.09091057  0.36940631 -0.74258198 -0.7854546 ]\n",
      " [-0.08384138  0.6059603   0.55190831  0.07959199  0.11448039]\n",
      " [-0.24566894  0.37446278  0.16476186 -0.91395312 -0.27462437]]\n",
      "\n",
      "Output of the second layer: \n",
      "[[ 0.07136211  0.01831101]\n",
      " [-0.05427832 -0.0786683 ]\n",
      " [ 0.07924331 -0.07230373]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output of the first layer: \\n{l1.output}\\n\\nOutput of the second layer: \\n{l2.output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the ReLU (Rectified Linear Unit) Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the ReLU activation function: [0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]   # Input data\n",
    "output = []\n",
    "\n",
    "for i in inputs:    # ReLU activation function\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "print(f\"Output of the ReLU activation function: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the ReLU activation function: [0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))    # ReLU activation function using max() function\n",
    "\n",
    "print(f\"Output of the ReLU activation function: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential values: [array([121.51041752,   3.35348465,  10.85906266]), array([7.33197354e+03, 1.63654137e-01, 1.22140276e+00]), array([4.0959554 , 2.8605102 , 1.02634095])]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [[4.8, 1.21, 2.385], [8.9, -1.81, 0.2], [1.41, 1.051, 0.026]]\n",
    "exp_values = []\n",
    "\n",
    "for i in layer_outputs:\n",
    "    exp_values.append(np.exp(i))\n",
    "    \n",
    "print(\"Exponential values: {}\".format(exp_values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values: [array([0.01629355, 0.52581832, 0.82850561]), array([0.98315722, 0.02566057, 0.09318843]), array([0.00054923, 0.44852111, 0.07830595])]\n"
     ]
    }
   ],
   "source": [
    "norm_base = sum(exp_values)\n",
    "\n",
    "norm_values = []\n",
    "for i in exp_values:\n",
    "    norm_values.append(i / norm_base)\n",
    "    \n",
    "print(\"Normalized values: {}\".format(norm_values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax includes two steps of operation which are normalization and exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax normalized values: \n",
      "[[0.895 0.025 0.08 ]\n",
      " [1.    0.    0.   ]\n",
      " [0.513 0.358 0.129]]\n"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(layer_outputs)\n",
    "normalized_exp = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "print(\"Softmax normalized values: \\n{}\".format(normalized_exp.round(3)))    # Rounded values after softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    exp_values = np.exp(inputs)\n",
    "    return exp_values / np.sum(exp_values, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer outputs after softmax activation: \n",
      "[[0.895 0.025 0.08 ]\n",
      " [1.    0.    0.   ]\n",
      " [0.513 0.358 0.129]]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [\n",
    "    [4.8, 1.21, 2.385], \n",
    "    [8.9, -1.81, 0.2], \n",
    "    [1.41, 1.051, 0.026]\n",
    "]\n",
    "layer_outputs_softmax = softmax(layer_outputs)\n",
    "print(\"Layer outputs after softmax activation: \\n{}\".format(layer_outputs_softmax.round(3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To prevent overflowing with rising exponential values, we can subtract the maximum value from the input (overflow prevention). With and without subtraction of the maximum value, the output of the softmax function is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)   # Random weights with the same shape as the input and the number of neurons.\n",
    "        self.biases = np.zeros((1, n_neurons))      # Biases with the shape of (1, n_neurons)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))     # Subtracting the maximum value from the inputs to avoid overflow.\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)      # Normalizing the values.\n",
    "        self.output = probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
